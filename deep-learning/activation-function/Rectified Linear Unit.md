- The **Rectified Linear Unit (ReLU)** $\sigma$ is an [activation function](Activation%20Function.md) that maps any real number to a value between 0 and 1.
$$
g(z) = z_+ = \max(0, z)
$$
where:
- $z$ is the input to the ReLU function
- $z_+$ is the positive part of $z$

The ReLU function is a piecewise linear function that maps any real number to a value between 0 and infinity. It is a popular activation function for deep learning models, particularly for neural networks with many layers.
